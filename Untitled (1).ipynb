{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport azureml.core\nfrom azureml.core import Workspace\n\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ws = Workspace.from_config()\nprint(ws.name, ws.location, ws.resource_group, sep='\\t')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.core import Experiment\nexperiment_name = 'Potato-chips-classification-version'\n\nexp = Experiment(workspace=ws, name=experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\ncompute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster-6s\")\ncompute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\ncompute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n\nvm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6S_V3\")\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n    min_nodes=compute_min_nodes,\n    max_nodes=compute_max_nodes)\n\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n    \n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \n    print(compute_target.get_status().serialize())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.core import Workspace, Dataset\n\ndata_folder = os.path.join(os.getcwd(), 'data')\nos.makedirs(data_folder, exist_ok=True)\n\nsubscription_id = '0c3a9701-fb22-41b3-b904-89b9960d70d6'\nresource_group = 'Koichi_1'\nworkspace_name = 'koichi_1'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\ndataset = Dataset.get_by_name(workspace, name='potatochips-data-dataset')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nscript_folder2 = os.path.join(os.getcwd(), 'potatochips_script')\nos.makedirs(script_folder2, exist_ok=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile $script_folder2/train.py\n\nimport argparse\nimport os\nimport numpy as np\n\nfrom azureml.core import Run\n\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import optimizers\nimport matplotlib.pyplot as plt\nimport random\nimport os\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\nparser.add_argument('--optimizer', type=str, dest='arg_optimizer', help='optimizer')\nparser.add_argument('--monitor', type=str, dest='arg_monitor', help='Learning_rate_reduction_monitor')\nparser.add_argument('--batchsize', type=int, dest='arg_batchsize', help='batchsize')\nparser.add_argument('--epochs', type=int, dest='arg_epochs', help='epochs')\nparser.add_argument('--Learning_rate', type=float, dest='arg_Learning_rate', help='Learning_rate')\nargs = parser.parse_args()\n\narg_optimizer = args.arg_optimizer\narg_monitor = args.arg_monitor\narg_batchsize = args.arg_batchsize\narg_epochs = args.arg_epochs\narg_Learning_rate = args.arg_Learning_rate\n\ndata_folder = args.data_folder\nprint('Data folder:', data_folder)\n\ntraining_folder = os.path.join(data_folder, \"potato-chips\")\nprint('Training folder:', training_folder)\n\nif arg_optimizer == \"SGD\":\n  optimizer = optimizers.SGD(lr=arg_Learning_rate)\nelif arg_optimizer == \"RMSprop\":\n  optimizer = optimizers.RMSprop(lr=arg_Learning_rate)\nelif arg_optimizer == \"Adam\":\n  optimizer = optimizers.Adam(lr=arg_Learning_rate)\nelse:\n  optimizer = 'rmsprop'\n\nFAST_RUN = False\nIMAGE_WIDTH = 192\nIMAGE_HEIGHT = 256\nIMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\nIMAGE_CHANNELS = 3\n\nfilenames = os.listdir(training_folder)\ncategories = []\nfor filename in filenames:\n  category = filename.split('.')[0]\n  categories.append(category)\n  \ndf = pd.DataFrame({\n  'filename' : filenames,\n  'category' : categories\n})\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(7, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\nmodel.summary()\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nearlystop = EarlyStopping(patience=10)\nlearning_rate_reduction = ReduceLROnPlateau(monitor=arg_monitor,\n                                            patience=2,\n                                            verbose=1,\n                                            factor=0.5,\n                                            min_lr=0.00001)\n\n# callbacks = [earlystop, learning_rate_reduction]\ncallbacks = [learning_rate_reduction]\n\ntrain_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)\ntrain_df = train_df.reset_index(drop=True)\nvalidate_df = validate_df.reset_index(drop=True)\n\ntotal_train = train_df.shape[0]\ntotal_validate = validate_df.shape[0]\nbatch_size=arg_batchsize\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    rescale=1./255,\n    shear_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df,\n    training_folder,\n    x_col='filename',\n    y_col='category',\n    target_size=IMAGE_SIZE,\n    class_mode='categorical',\n    batch_size=batch_size,\n    validate_filenames=False\n)\n\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\nvalidation_generator = validation_datagen.flow_from_dataframe(\n    validate_df,\n    training_folder,\n    x_col='filename',\n    y_col='category',\n    target_size=IMAGE_SIZE,\n    class_mode='categorical',\n    batch_size=batch_size,\n    validate_filenames=False\n)\n\nrun = Run.get_context()\n\nepochs = 3 if FAST_RUN else arg_epochs\nhistory = model.fit_generator(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=total_validate//batch_size,\n    steps_per_epoch=total_train//batch_size,\n    callbacks=callbacks\n)\n\n\nrun.log('optimizer', arg_optimizer)\nrun.log('monitor', arg_monitor)\nrun.log('batchsize', arg_batchsize)\nrun.log('epochs', arg_epochs)\nrun.log('accuracy', history.history['accuracy'][-1])\nrun.log('val_accuracy', history.history['val_accuracy'][-1])\nrun.log('loss', history.history['loss'][-1])\nrun.log('val_loss', history.history['val_loss'][-1])\n\nval_accuracy = history.history['val_accuracy'][-1]\n\nfrom azureml.core.run import Run\nrun_logger = Run.get_context()\nrun_logger.log(\"accuracy\", float(val_accuracy))\n\noutput_folder = os.path.join(os.getcwd(), \"outputs\")\nos.makedirs(output_folder, exist_ok=True)\n\nmodel.save_weights(os.path.join(os.getcwd(),\"model.h5\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.copy('utils.py', script_folder)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nenv = Environment('potato-env')\ncd = CondaDependencies.create(pip_packages=['azureml-defaults', 'tensorflow', 'matplotlib'],conda_packages=['scikit-learn==0.22.1'])\n\nenv.python.conda_dependencies = cd\nenv.register(workspace=ws)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.train.hyperdrive import RandomParameterSampling\nfrom azureml.train.hyperdrive import choice, loguniform\n\nparam_sampling = RandomParameterSampling( {\n    \"--optimizer\": choice('SGD', 'RMSprop', 'Adam'),\n    \"--monitor\": choice('val_accuracy', 'val_loss'),\n    \"--batchsize\": choice(16, 32, 64),\n    \"--epochs\": choice(30, 50, 70, 90, 120, 150),\n    \"--Learning_rate\": loguniform(-4, -1)\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.train.hyperdrive import PrimaryMetricGoal\nprimary_metric_name=\"accuracy\",\nprimary_metric_goal=PrimaryMetricGoal.MAXIMIZE","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.train.hyperdrive import BanditPolicy\nearly_termination_policy = BanditPolicy(slack_factor = 0.1, evaluation_interval=1, delay_evaluation=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.core import ScriptRunConfig\n\nargs = ['--data-folder', dataset.as_mount()]\n\nsrc = ScriptRunConfig(source_directory=script_folder2,\n                                        script='train.py',\n                                        arguments=args,\n                                        compute_target=compute_target,\n                                        environment=env)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.train.hyperdrive import HyperDriveConfig\nhd_config = HyperDriveConfig(run_config=src,\n                            hyperparameter_sampling=param_sampling,\n                            policy=early_termination_policy,\n                            primary_metric_name='accuracy',\n                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                            max_total_runs=100,\n                            max_concurrent_runs=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.core.experiment import Experiment\nexperiment = Experiment(workspace, experiment_name)\nhyperdrive_run = experiment.submit(hd_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from azureml.widgets import RunDetails\nRunDetails(hyperdrive_run).show()","metadata":{},"execution_count":null,"outputs":[]}]}